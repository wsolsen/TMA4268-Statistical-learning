---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 2: Group 4"
author: "Ã˜ystein Alvestad, Lars Evje and William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=5, fig.height=4)

# Import packages
library(magrittr)
library(dplyr)
library(tidyverse)
library(MASS)
library(yardstick)
library(class)
library(pROC)
library(e1071)
library(leaps)
library(ggplot2)
library(glmnet)
```

\newcommand{\E}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\bfbeta}{\boldsymbol{\beta}}

\newcommand{\Hquad}{\hspace{0.5em}}

```{r rpackages, eval=FALSE,echo=FALSE}
install.packages("knitr")  #probably already installed
install.packages("rmarkdown")  #probably already installed
install.packages("ggplot2")  #plotting with ggplot
install.packages("ggfortify")
install.packages("leaps")
install.packages("glmnet")
install.packages("tree")
install.packages("caret")
install.packages("randomForest")
install.packages("readr")
install.packages("e1071")
install.packages("dplyr")
install.packages("gbm")
```



# Problem 1
## a)

TRUE, TRUE, TRUE, FALSE

## b)

Read the file:
```{r, echo=TRUE, eval=TRUE}
id <- "1iI6YaqgG0QJW5onZ_GTBsCvpKPExF30G" # google file ID
catdat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                   header = T)
```

Check if the dataset contains any Nans or infinite values:
```{r, echo=TRUE, eval=TRUE}
# Check if data contains Nans
any(apply(catdat, 2, function(x) any(is.na(x))))
any(apply(catdat, 2, function(x) any(is.infinite(x))))
```

We proceed by splitting the data into training and test sets. Afterwards we use regsubsets to search for the best subset models. To be able to decide which model is the best we look at three different model selection criteria: adjusted-$R^2$, Mallows' Cp and BIC. The plot shows these model selection criteria as a function of the number of features. 

The red dot indicates the best value of the respective model selection criterion. We see that according to the BIC a model with 6 features is preferred, while Cp and adjusted-$R^2$ do well for 6 features, but even better for higher number of features. To gain robustness in the model we select a model with 6 parameters.

```{r, echo=TRUE, eval=TRUE,fig.width=6, fig.height=4}

# Split data set into train and test
set.seed(4268)
train.ind = sample(1:nrow(catdat), 0.5 * nrow(catdat))
catdat.train = catdat[train.ind, ]
catdat.test = catdat[-train.ind, ]

# Fit many models in search for best subset models
regfit.full = regsubsets(birds ~ . , catdat.train, nvmax=16)

# Extract indices for best min and max values of errors
adjr2_max <- which.max(summary(regfit.full)$adjr2)
cp_min <- which.min(summary(regfit.full)$cp)
bic_min <- which.min(summary(regfit.full)$bic)

# Plot 
par(mfrow=c(2,2), mar=c(5.1,4.1,1,1))
plot(summary(regfit.full)$rss, xlab="Number of Features", ylab="RSS", type="l")
plot(summary(regfit.full)$adjr2, xlab="Number of Features", ylab="Adjusted RSq", type="l")
points(adjr2_max, summary(regfit.full)$adjr2[adjr2_max], col="red" , cex=2 , pch=20)
plot(summary(regfit.full)$cp, xlab="Number of Features", ylab="Cp", type="l")
points(cp_min, summary(regfit.full)$cp[cp_min], col="red" , cex=2 , pch=20)
plot(summary(regfit.full)$bic, xlab="Number of Features", ylab="BIC", type="l")
points(bic_min, summary(regfit.full)$bic[bic_min], col="red" , cex=2 , pch=20)
```

For a linear model with six features we find the six best feautres by reading off the regsubsets-summary. The features chosen are: wetfood, daily.playtime, children.13, urban, bell and daily.outdoortime. From the understanding of the domain, the selected features all seem reasonable with respect to having predictive power. We check the model qualitatively by plotting predicted values for the training set and test set against the response in the two plots below. We see that the model performs well.

```{r, echo=TRUE, eval=TRUE,fig.width=6, fig.height=4}
# Fit model with 6 parameters
lm_6 = lm(birds ~ wetfood+daily.playtime+children.13+urban+bell+daily.outdoortime, catdat.train)

# Predict and calculate MSE
lm_birds_hat <- predict(lm_6, catdat.test[c("wetfood", "daily.playtime", "children.13", "urban", "bell", "daily.outdoortime")])
birds_mse <- mean((catdat.test[[c("birds")]] - lm_birds_hat)^2)

lm_birds_hat_train <- predict(lm_6, catdat.train[c("wetfood", "daily.playtime", "children.13", "urban", "bell", "daily.outdoortime")])

plot.new()
par(mfrow=c(1,1))
plot(catdat.test[[c("birds")]], lm_birds_hat, xlim=c(0,400), ylim=c(0,400), xlab="birds test", ylab="birds predicted")

plot.new()
par(mfrow=c(1,1))
plot(catdat.train[[c("birds")]], lm_birds_hat_train, xlim=c(0,400), ylim=c(0,400), , xlab="birds train", ylab="birds predicted")
```

The test MSE for a linear model with 6 parameters (list above) is: `r birds_mse`.

## c)

We use glmnet to perform Lasso regression. $\lambda$ is chosen through cross validation which is offered as part of the glm-package. The plot below shows how MSE varies with different values of $\lambda$. 

The non-zero coefficients are: weight, dryfood, wetfood, owner.income, daily.playtime, owner.age, house.area, children.13, urban, bell, dogs, daily.outdoortime, neutered.

```{r, echo=TRUE, eval=TRUE,fig.width=6, fig.height=4}
# Split train and test into feature and response parts
x.train <- model.matrix(birds ~ ., data = catdat.train)[, -1]
y.train <- catdat.train$birds
x.test = model.matrix(birds ~ ., data = catdat.test)[, -1]
y.test = catdat.test$birds

# Find best lambda and fit model
set.seed(4268)
cv.out = cv.glmnet(x.train, y.train, alpha=1)
plot(cv.out, xlim=c(0, 5))
bestlam = cv.out$lambda.min

lasso.mod = glmnet(x.train, y.train, alpha=1, lambda=bestlam)
lasso.pred = predict(lasso.mod, s=bestlam, newx=x.test)
lasso_birds_mse <- mean((catdat.test[[c("birds")]] - lasso.pred)^2)

# Show coefficients of Lasso model
coef(lasso.mod, s = bestlam)
```

The test MSE for the best Lasso-model is: `r lasso_birds_mse`.

## d)

If we let $\lambda \rightarrow \infty$ the best fit will have all coefficients $\beta_1, \beta_2, ...$ equal to zero. This is because the regularizing term in the objective will dominate the solution. Then the only coefficient left is the intercept $\beta_0$ which optimal value will be equal to the average of the response values of the training examples.

If we let $\lambda = 0$ the regularizing term disappears from the objective. Then we are left with the least squares objective from regular linear regression.

```{r, echo=TRUE, eval=TRUE,fig.width=6, fig.height=4}
# Extreme lasso values
lasso.mod_lamzero = glmnet(x.train, y.train, alpha=1, lambda=0)
lasso.pred_lamzero = predict(lasso.mod_lamzero, s=0, newx=x.test)
lasso_birds_mse_lamzero <- mean((catdat.test[[c("birds")]] - lasso.pred_lamzero)^2)

lasso.mod_lambig = glmnet(x.train, y.train, alpha=1, lambda=1e10)
lasso.pred_lambig = predict(lasso.mod_lambig, s=1e10, newx=x.test)
lasso_birds_mse_lambig <- mean((catdat.test[[c("birds")]] - lasso.pred_lambig)^2)
```

The test-MSE for the Lasso-model with $\lambda = 0$ is: `r lasso_birds_mse_lamzero`.

The test-MSE for the Lasso-model with $\lambda \rightarrow \infty$ is: `r lasso_birds_mse_lambig`.

## e)

The model with only intercept is much worse than both the reduced six-parameter linear regression model and the best Lasso-regression model. The difference is about 1 order of magnitude in the MSE.

The full linear model (using all features) is about as good as the reduced linear model and the best Lasso-regression model. This is somewhat not surprising from the plots showing adjusted-$R^2$ and Mallows' Cp in problem 1 b) where we can see that the performance is quite stable for many features. As the BIC parameter showed (in the same plot) is however that we are better off choosing a smaller model with fewer features (which is preferable) although the performance is not much better for the smaller model.

```{r, echo=TRUE, eval=TRUE,fig.width=6, fig.height=4}
# Model with only intercept
intercept_mse <- mean((y.test - mean(y.train))^2)

# Full regression model
lm_full = lm(birds ~ ., catdat.train)

# Predict and calculate MSE
lmfull_birds_hat <- predict(lm_full, catdat.test)
lmfull_birds_mse <- mean((catdat.test[[c("birds")]] - lmfull_birds_hat)^2)
```

The test-MSE for the full linear model is: `r lmfull_birds_mse`.

The test-MSE for the only intercept model is: `r intercept_mse`.

## f)

The table below summarizes the results from the modeling above. 

|  Model |  MSE |
|---|---|
| Full linear model  |  `r lmfull_birds_mse` |
| Reduced linear model  |  `r birds_mse` |
| Best Lasso-regression model  |  `r lasso_birds_mse` |
| Intercept model  |  `r intercept_mse` |

A pure intercept model will only perform well for problems with no signal in the data. The F-test of e.g. the reduced linear model suggests that a null hypothesis of having all coefficients $\beta_1, \beta_2, ...$ equal to zero should be rejected. On this basis it is not unsurprising that the intercept model performs worse than the two other linear models. The best Lasso-regression model (optimized over many values of $\lambda$) lies on the spectrum of the intercept model and the full linear model. Thus, the Lasso-model is expected to to better than the intercept model in this case, because all small values of $\lambda$ will give something similar to a full linear model.

The reduced linear model performs better than the full liner model, but they are still within 1% of each other. So, there is not much gained with respect to. prediction to use a reduced linear model. It could however be beneficial if inference is of interest. The Lasso-regression does slightly better than the reduced linear model. So the bias in the estimation (introduced by the regularization) was dominated by the Lasso-regressions ability to search for a good reduced model. So Lasso proved slightly better than an analyst's effort. Again, with reference to the model criteria plotted, the Lasso-regression was able to search the constant interval of of e.g. adjusted-$R^2$ and find a better solution than the reduced linear model. However, since the interval is constant, it does not make much change in the end result.

# Problem 2

## a) 

TRUE, TRUE, FALSE, FALSE

## b)

The basis functions for a cubic spline with knots at the quartiles $q_1$ and $q_2$ of variable X are: $X$, $X^2$, $X^3$, $h(X, q_1)$, $h(X, q_2)$. The function $h(x, q)$ is defined as:

$$h(x, q) = \begin{cases} 
      (x-q)^3 & x > q \\
      0  & x \leq q
   \end{cases}$$

## c)


We fit polynomials of degree 1, 2, ..., 10 to model the relationship between daily outdoor time and birds killed. The first plot below shows the polynomial fit of degree 1 through 5 and the second plot shows the polynomial fit of degree 6 through 10. We have used different colors in the two plots for the different polynomials:

- Degree 1 and 6 are red
- Degree 2 and 7 are green
- Degree 3 and 8 are blue
- Degree 4 and 9 are yellow
- Degree 5 and 10 are black

In the daily outdoortime interval from 130 to 260 all polynomials are roughly linear. Approaching the lower and upper limits of the range the polynomials start to display non-linear behaviour.

```{r, echo=TRUE, eval=TRUE, out.width='\\textwidth'}

# Create fits d=1,2,...,10
fit1<-lm(birds ~ poly(daily.outdoortime, 1, raw=T) , data=catdat.train)
fit2<-lm(birds ~ poly(daily.outdoortime, 2, raw=T) , data=catdat.train)
fit3<-lm(birds ~ poly(daily.outdoortime, 3, raw=T) , data=catdat.train)
fit4<-lm(birds ~ poly(daily.outdoortime, 4, raw=T) , data=catdat.train)
fit5<-lm(birds ~ poly(daily.outdoortime, 5, raw=T) , data=catdat.train)
fit6<-lm(birds ~ poly(daily.outdoortime, 6, raw=T) , data=catdat.train)
fit7<-lm(birds ~ poly(daily.outdoortime, 7, raw=T) , data=catdat.train)
fit8<-lm(birds ~ poly(daily.outdoortime, 8, raw=T) , data=catdat.train)
fit9<-lm(birds ~ poly(daily.outdoortime, 9, raw=T) , data=catdat.train)
fit10<-lm(birds ~ poly(daily.outdoortime, 10, raw=T) , data=catdat.train)

# Create grid and predictions d=1,2,...,10
pred_grid <- data.frame(daily.outdoortime =seq(min(catdat.train[[c("daily.outdoortime")]]), 
                                               max(catdat.train[[c("daily.outdoortime")]]), 
                                               length.out = 100))

pred1 <- predict(fit1, newdata=pred_grid)
pred2 <- predict(fit2, newdata=pred_grid)
pred3 <- predict(fit3, newdata=pred_grid)
pred4 <- predict(fit4, newdata=pred_grid)
pred5 <- predict(fit5, newdata=pred_grid)
pred6 <- predict(fit6, newdata=pred_grid)
pred7 <- predict(fit7, newdata=pred_grid)
pred8 <- predict(fit8, newdata=pred_grid)
pred9 <- predict(fit9, newdata=pred_grid)
pred10 <- predict(fit10, newdata=pred_grid)

# Plot polynomials degree 1 through 5
{plot(catdat.train[[c("daily.outdoortime")]], catdat.train[[c("birds")]], xlab="daily outdoortime", ylab="birds")
lines(x = pred_grid[["daily.outdoortime"]], y = pred1, col="red")
lines(x = pred_grid[["daily.outdoortime"]], y = pred2, col="green")
lines(x = pred_grid[["daily.outdoortime"]], y = pred3, col="blue")
lines(x = pred_grid[["daily.outdoortime"]], y = pred4, col="yellow")
lines(x = pred_grid[["daily.outdoortime"]], y = pred5)}

# Plot polynomials degree 6 through 10
{plot(catdat.train[[c("daily.outdoortime")]], catdat.train[[c("birds")]], xlab="daily outdoortime", ylab="birds")
lines(x = pred_grid[["daily.outdoortime"]], y = pred6, col="red")
lines(x = pred_grid[["daily.outdoortime"]], y = pred7, col="green")
lines(x = pred_grid[["daily.outdoortime"]], y = pred8, col="blue")
lines(x = pred_grid[["daily.outdoortime"]], y = pred9, col="yellow")
lines(x = pred_grid[["daily.outdoortime"]], y = pred10)}

```

We see a clear improvement in going from a 1st degree polynomial to a 2nd degree polynomial. The second degree polynomial catches the increase in birds killed at high values of daily outdoor time. Further increases in the polynomial degree leads to a slow decrease in MSE, but no notable improvements. Higher degree polynomials can do no worse in their predictions than lower degree polynomials, but whatever approvement in MSE above degree 2 is probably only overfitting.

|  Model |  MSE training |
  |---|---|
  | 1st degree  |  `r sigma(fit1)^2 * 224/226` |
  | 2nd degree  |  `r sigma(fit2)^2 * 223/226` |
  | 3rd degree  |  `r sigma(fit3)^2 * 222/226` |
  | 4th degree  |  `r sigma(fit4)^2 * 221/226` |
  | 5th degree  |  `r sigma(fit5)^2 * 220/226` |
  | 6th degree  |  `r sigma(fit6)^2 * 219/226` |
  | 7th degree  |  `r sigma(fit7)^2 * 218/226` |
  | 8th degree  |  `r sigma(fit8)^2 * 217/226` |
  | 9th degree  |  `r sigma(fit9)^2 * 216/226` |
  | 10th degree |  `r sigma(fit10)^2 * 215/226`|

# Problem 3
## a)


## b)


## c)
### (i)


### (ii)




# Problem 4
## a)
False, True, False, True.

<!-- 
Kort forklaring:
i) Dataen vil nok kunne vÃ¦re ikke-separabel.
ii) En outlier vil kunne ha stor innvirkning pÃ¥ margin.
iii) If class boundaries are non-linear then SVM is more popular, but kernel versions of logistic regression are also possible, but more computationally expensive (and traditionally less used).
iv) Big C => big margin => lower variance and bigger bias.
-->

## b)
### (i)
A SVM tends to behave better than logistic regression when the classes are well separated, which we would assume them to be here with the genomic data. Also SVM has the ability to work in high dimensional space compared to the small number of samples.

Instead of SVM one could for example use a random forest, $K$-nearest neighbor, linear classifiers, or quadratic classifiers.


### (ii)
The paper introduces an ensemble SVM-Recursive Feature Elimination for gene selection that follows the concept of ensemble and bagging used in random forest but adopts the backward elimination strategy which is the rationale of Recursive Feature Elimination algorithm.


### (iii)
In the following code block we fit a support vector classifier with $C=1$ on `Category` using `d.leukemia.train`.

```{r SVC}

set.seed(2399)

# Set-up:
id <- "1x_E8xnmz9CMHh_tMwIsWP94czPa1Fpsj"  # Google file ID
path <- "https://docs.google.com/uc?id=%s&export=download"
d.leukemia <- read.csv(sprintf(path, id), header = TRUE)

t.samples <- sample(1:60, 15, replace = FALSE)
d.leukemia$Category <- as.factor(d.leukemia$Category)
d.leukemia.test <- d.leukemia[t.samples, ]
d.leukemia.train <- d.leukemia[-t.samples, ]

# Support vector classifier:
svmfit <- svm(Category ~ ., data = d.leukemia.train,
              kernel = "linear", cost = 1, scale = TRUE)
pred_train <- predict(svmfit, d.leukemia.train)
pred_test <- predict(svmfit, d.leukemia.test)

# Confusion tables for training and testing respectively:
conf_tab_train <- table(predict = pred_train, truth = d.leukemia.train$Category)
conf_tab_test <- table(predict = pred_test, truth = d.leukemia.test$Category)

# Misclassification for training and testing respectively:
misclas_train <- 1 - sum(diag(conf_tab_train)) / sum(conf_tab_train)
misclas_test <- 1 - sum(diag(conf_tab_test)) / sum(conf_tab_test)

```

We then see the confusion table for the training data below, with the misclassification error rate of `r misclas_train`.
```{r}
conf_tab_train
```
We also have the confusion table for the test data below, with the misclassification error rate of `r misclas_test`.
```{r}
conf_tab_test
```

The training error rate is `r misclas_train`, suggesting that there is an overfitting of the data. This is dependent on the cost $C$, and one could have done a cross validation to find a possibly better cost than $C=1$.

The most common error in the test set is that the truth is relapse, while the prediction is non-relapse. That is, children relapse even though the prediction is that they do not. With a misclassification error rate of `r misclas_test` for the test set the classification can be said to be successful. However, the false positive, which is the most common error, is worse than the false negative in this case, in our opinion.


### (iv)
In the following code block we fit a support vector machine to the data using the cost $C=1$ and the tuning parameter $\gamma=10^{-2}$ or $\gamma=10^{-5}$.

```{r SVM}

set.seed(2399)

# Support vector machine and prediction:
svmfit_gamma1 <- svm(Category ~ ., data = d.leukemia.train, kernel = "radial",
                     cost = 1, gamma = 1e-2, scale = TRUE)
svmfit_gamma2 <- svm(Category ~ ., data = d.leukemia.train, kernel = "radial",
                     cost = 1, gamma = 1e-5, scale = TRUE)
pred_train_gamma1 <- predict(svmfit_gamma1, d.leukemia.train)
pred_test_gamma1 <- predict(svmfit_gamma1, d.leukemia.test)
pred_train_gamma2 <- predict(svmfit_gamma2, d.leukemia.train)
pred_test_gamma2 <- predict(svmfit_gamma2, d.leukemia.test)

# Confusion tables for training and testing:
conf_tab_train_gamma1 <- table(predict = pred_train_gamma1,
                               truth = d.leukemia.train$Category)
conf_tab_test_gamma1 <- table(predict = pred_test_gamma1,
                              truth = d.leukemia.test$Category)
conf_tab_train_gamma2 <- table(predict = pred_train_gamma2,
                               truth = d.leukemia.train$Category)
conf_tab_test_gamma2 <- table(predict = pred_test_gamma2,
                              truth = d.leukemia.test$Category)

# Misclassification for training and testing:
misclas_train_gamma1 <- 1 - sum(diag(conf_tab_train_gamma1)) / sum(conf_tab_train_gamma1)
misclas_test_gamma1 <- 1 - sum(diag(conf_tab_test_gamma1)) / sum(conf_tab_test_gamma1)
misclas_train_gamma2 <- 1 - sum(diag(conf_tab_train_gamma2)) / sum(conf_tab_train_gamma2)
misclas_test_gamma2 <- 1 - sum(diag(conf_tab_test_gamma2)) / sum(conf_tab_test_gamma2)

```

We then see the confusion table for the training data for $\gamma = 10^{-2}$ below, with the misclassification error rate of `r misclas_train_gamma1`.
```{r}
conf_tab_train_gamma1
```
We also have the confusion table for the test data for $\gamma = 10^{-2}$ below, with the misclassification error rate of `r misclas_test_gamma1`.
```{r}
conf_tab_test_gamma1
```

For $\gamma = 10^{-5}$ er have the confusion table for the training data below, with the misclassification error rate of `r misclas_train_gamma2`.
```{r}
conf_tab_train_gamma2
```
We also have the confusion table for the test data for $\gamma = 10^{-5}$ below, with the misclassification error rate of `r misclas_test_gamma2`.
```{r}
conf_tab_test_gamma2
```

We note that the misclassification error rate for the training set is `r misclas_train_gamma1` for $\gamma = 10^{-2}$ and `r misclas_train_gamma2` for $\gamma = 10^{-5}$. This can be explained by the fact that for small $\gamma$ the decision boundaries are smoother than for larger $\gamma$. Thus, there may be some overfitting for $\gamma = 10^{-2}$. For the test data however, the results are the same. Comparing to the case in (iii), the results are worse, suggesting that the support vector classifier is better than the support vector machine for this dataset.



## c)
The polynomial kernel of positive integer degree $d$ has the form
$$
  K(\bfx, \bfy) = (1 + \bfx^\top \bfy)^d = \left( 1 + \sum_{i=1}^p x_{i} y_{i} \right)^d,
$$
for $\bfx, \bfy \in \RR^p$, with elements $x_i$ and $y_i$ for $i=1,\dots,p$. We assume $d=2$ and $\bfx, \bfy \in \RR^2$, such that
\begin{align*}
  K(\bfx, \bfy) &= (1 + \bfx^\top \bfy)^2 = 1 + 2 \bfx^\top \bfy + (\bfx^\top \bfy)^2 = 1 + 2 (x_1 y_1 + x_2 y_2) + (x_1 y_1 + x_2 y_2)^2
  \\
  &= 1 + 2 x_1 y_1 + 2 x_2 y_2 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 x_2 y_2.
\end{align*}
We then see that
$$
  K(\bfx, \bfy) = \bfh(\bfx)^\top \bfh(\bfy) = \left< \bfh(\bfx), \bfh(\bfy) \right>,
$$
by the basic definition of the inner product of two vectors, where,
$$
  \bfh(\bfx) =
  \begin{bmatrix}
    1
    \\
    \sqrt{2} x_1
    \\
    \sqrt{2} x_2
    \\
    x_1^2
    \\
    x_2^2
    \\
    \sqrt{2} x_1 x_2
  \end{bmatrix}
  \quad \text{and} \quad
  \bfh(\bfy) =
  \begin{bmatrix}
    1
    \\
    \sqrt{2} y_1
    \\
    \sqrt{2} y_2
    \\
    y_1^2
    \\
    y_2^2
    \\
    \sqrt{2} y_1 y_2
  \end{bmatrix}.
$$



# Problem 5
## a)
True, False, False, False.

<!-- 
Kort forklaring:
i) The second principal component is the direction which maximizes variance among all directions orthogonal to the first.
ii) MÃ¥ vÃ¦re standarisert.
iii) K-means algoritmen finner lokalt og ikke globalt optimum, forskjellige initialverdier kan derfor gi forskjellige lÃ¸sninger. BÃ¸r kjÃ¸res flere ganger med forskjellige initialverdier.
iv) Greien med PCA er Ã¥ lage en uncorrelated basis.
-->

## b)
In the following we make a random cluster of the data and compute the centroid of the two clusters we get. The clusters are color coded where one cluster is colored red, while the other is green, as seen in Figure \ref{fig:RandomClustering}. Note that the code here is not general for every $K$-mean clustering, but is only applicable to $K=2$, which is the case given in the problem.

```{r K-means clustering random, fig.cap="\\label{fig:RandomClustering}A random clustering of the data being the round points, and the centroids being the square points."}
set.seed(1)

x1 <- c(1, 2, 0, 4, 5, 6)
x2 <- c(5, 4, 3, 1, 1, 2)
X <- matrix(c(x1, x2), ncol = 2)

# Random cluster
X_cluster <- cbind(X, sample(c(1, 2), size = nrow(X), replace = TRUE))

# Initializing and computing the centroids:
g1_centroid <- c(0, 0)
g2_centroid <- c(0, 0)

for(i in 1:length(x1)) {
  if(X_cluster[i, 3] == 1) {
    g1_centroid[1] <- g1_centroid[1] + X[i, 1]
    g1_centroid[2] <- g1_centroid[2] + X[i, 2]
  }
  else {
    g2_centroid[1] <- g2_centroid[1] + X[i, 1]
    g2_centroid[2] <- g2_centroid[2] + X[i, 2]
  }
}

g1_centroid <- g1_centroid / length((X[, 1])[X_cluster[, 3] == 1])
g2_centroid <- g2_centroid / length((X[, 1])[X_cluster[, 3] == 2])


# Plotting the clusters and centroids color coded:
plot(X,
     col = X_cluster[, 3] + 1,
     main = "Random custering of the data with the centroids",
     xlab = "x1",
     ylab = "x2",
     pch = 20,
     cex = 2)
points(g1_centroid[1], g1_centroid[2], pch = 15, cex = 2, col = 2)
points(g2_centroid[1], g2_centroid[2], pch = 15, cex = 2, col = 3)
```

We can then measure, using the Euclidean distance, what points are closest to the respective centroids, in this case giving the correct clustering for $K=2$. This is shown in Figure \ref{fig:Clustering}.

```{r K-means clustering, fig.cap="\\label{fig:Clustering}The $K$-means clustering for the data, with $K=2$."}
dist <- function(x, y) {
  return(sqrt(sum((x - y)^2)))
}

for(i in 1:length(x1)) {
  X_cluster[i, 3] <- ifelse(dist(g1_centroid, X[i, ]) < dist(g2_centroid, X[i, ]), 1, 2)
}

plot(X,
     col = X_cluster[, 3] + 1,
     main = "K-means custering of the data with K = 2",
     xlab = "x1",
     ylab = "x2",
     pch = 20,
     cex = 2)
```


## c)


## d)


## e)
### (i)


### (ii)


## f)


