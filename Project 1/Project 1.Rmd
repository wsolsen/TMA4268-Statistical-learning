---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 4"
author: "Ã˜ystein Alvestad, Lars Evje and William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=4, fig.height=3)

# Import packages
library(magrittr)
library(dplyr)
library(tidyverse)
library(MASS)
library(yardstick)
library(class)
library(pROC)
```

\newcommand{\E}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\bfbeta}{\boldsymbol{\beta}}

\newcommand{\Hquad}{\hspace{0.5em}}

```{r rpackages, eval=FALSE,echo=FALSE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("ggplot2") #plotting with ggplot
install.packages("ggfortify")  
install.packages("MASS")
install.packages("class")
install.packages("pROC")
install.packages("plotROC")
```



# Problem 1
We consider the regression problem
$$
  Y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon,
$$
where $\E(\varepsilon)=0$ and $\Var(\varepsilon)=\sigma^2$. We define $\bfx, \bfbeta \in \RR^{p+1}$ such that $f(\bfx) = \bfx^\top \bfbeta = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$.

## a)
We consider the estimator
$$
  \tilde{\bfbeta} = (X^\top X + \lambda I)^{-1} X^\top \bfY,
$$
for $\bfbeta$. Here $X$ is the design matrix, $\bfY$ is the response vector, $I$ is the identity matrix, and $\lambda \geq 0$ is a constant. We recall that for a constant matrix $A$ of appropriate dimensions, and a random vector $\bfZ$, we have that
\begin{equation} \label{eq:RulesForExpectationAndCovOfMatrix}
  \E(A \bfZ) = A \E(\bfZ)
  \quad \text{and} \quad
  \Var(A \bfZ) = A \E(\bfZ) A^\top.
\end{equation}
First we now find the expected value
$$
  \E(\tilde{\bfbeta}) = \E((X^\top X + \lambda I)^{-1} X^\top \bfY) = (X^\top X + \lambda I)^{-1} X^\top \E(\bfY),
$$
and because $\E(\bfY) = X \bfbeta$, we get that
$$
  \E(\tilde{\bfbeta}) =  (X^\top X + \lambda I)^{-1} X^\top X \bfbeta.
$$
Similarly, the variance-covariance matrix is
$$
  \Var(\tilde{\bfbeta}) = \Var((X^\top X + \lambda I)^{-1} X^\top \bfY) = (X^\top X + \lambda I)^{-1} X^\top \Var(\bfY) [(X^\top X + \lambda I)^{-1} X^\top]^\top,
$$
and because $\Var(\bfY) = \sigma^2 I$, we get that
$$
  \Var(\tilde{\bfbeta}) = \sigma^2 (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1},
$$
where we used $(B^{-1})^\top = (B^\top)^{-1}$, for an invertible matrix $B$.


## b)
We now let $\tilde{f}(\bfx_0) = \bfx_0^\top \tilde{\bfbeta}$ be the prediction at a new covariate vector $\bfx_0$, and we wish the expected value and variance of this. Using what we learned in **a)** and Equation \eqref{eq:RulesForExpectationAndCovOfMatrix}, we find that
$$
  \E(\tilde{f}(\bfx_0)) = \E(\bfx_0^\top \tilde{\bfbeta}) = \bfx_0^\top \E(\tilde{\bfbeta}) = \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta.
$$
Similarly, we find that
\begin{equation} \label{eq:variance}
  \Var(\tilde{f}(\bfx_0)) = \Var(\bfx_0^\top \tilde{\bfbeta}) = \bfx_0^\top \Var(\tilde{\bfbeta}) \bfx_0 = \sigma^2 \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \bfx_0.
\end{equation}


## c)
We want to find the expected MSE at $\bfx_0$, and this is most easily done using the relationship between the expected value and the variance-covariance matrix,
$$
  \E((y_0 - \tilde{f}(\bfx_0))^2) = \E(\tilde{f}(\bfx_0) - f(\bfx_0))^2 + \Var(\tilde{f}(\bfx_0)) + \Var(\varepsilon).
$$
The last two terms are known to us now, so we look further at the first term, the squared bias,
\begin{equation} \label{eq:squaredBias}
  \begin{split}
    \E(\tilde{f}(\bfx_0) - f(\bfx_0))^2 &= [\E(\tilde{f}(\bfx_0)) - \E(f(\bfx_0))]^2 = [\E(\tilde{f}(\bfx_0)) - f(\bfx_0)]^2
    \\
    &= [\bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta - \bfx_0^\top \bfbeta]^2.
  \end{split}
\end{equation}
We then get that
\begin{align*}
  \E((y_0 - \tilde{f}(\bfx_0))^2) &= \sigma^2 + [\bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta - \bfx_0^\top \bfbeta]^2
  \\
  &\phantom{=} + \sigma^2 \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \bfx_0.
\end{align*}


## d)
We start by importing the relevant quantities, as given in the project description.
```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # Google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

X <- values$X
x0 <- values$x0
beta <- values$beta
sigma <- values$sigma
```
We may then calculate the squared bias $\E(\tilde{f}(\bfx_0) - f(\bfx_0))^2$, using Equation \eqref{eq:squaredBias}. Plotting this for $\lambda \in [0, 2]$, we get the result as in Figure \ref{fig:bias}. We see that the squared bias increases with large $\lambda > 0.5$, and has a minimum when $\lambda = 0$ and $\lambda \approx 0.5$. This is expected for $\lambda = 0$, because we then get that $\tilde{\bfbeta}$ is equal to the OLS estimator. The bias measures how good the estimator is in estimating the real value, so this makes sense. From the figure it also appears that for $\lambda \approx 0.5$, the estimator $\tilde{\bfbeta}$ us estimating the real value $\bfbeta$ very good. For increasing $\lambda$ after this, the estimator seems to do a worse job in estimating $\bfbeta$.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:bias}The squared bias $\\E(\\tilde{f}(\\bfx_0) - f(\\bfx_0))^2$ as a function of $\\lambda$."}

bias <- function(lambda, X, x0, beta) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- (t(x0) %*% inv %*% t(X) %*% X %*% beta - t(x0) %*% beta)^2
  return(value)
}

lambdas <- seq(0, 2, length.out = 500)
BIAS <- rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  BIAS[i] <- bias(lambdas[i], X, x0, beta)
}
dfBias <- data.frame(lambdas = lambdas, bias = BIAS)

ggplot(dfBias, aes(x = lambdas, y = bias)) +
  geom_line(color = "red") +
  xlab(expression(lambda)) +
  ylab(expression(bias^2))
```


## e)
We are now interested in $\Var(\tilde{f}(\bfx_0))$, which can be calculated using Equation \eqref{eq:variance}. Using $\lambda \in [0, 2]$, the result is plotted in Figure \ref{fig:variance}. We notice that the variance is decreasing for increasing values of $\lambda$. That is, as long as $\lambda$ increases, the model is less prone to overfitting. Letting $\lambda \to \infty$, we also see that the variance tends to zero.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:variance}The variance $\\Var(\\tilde{f}(\\bfx_0))$ as a function of $\\lambda$."}
variance <- function(lambda, X, x0, sigma) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- sigma^2 * t(x0) %*% inv %*% t(X) %*% X %*% inv %*% x0
  return(value)
}

lambdas <- seq(0, 2, length.out = 500)
VAR <- rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  VAR[i] <- variance(lambdas[i], X, x0, sigma)
}
dfVar <- data.frame(lambdas = lambdas, var = VAR)

ggplot(dfVar, aes(x = lambdas, y = var)) +
  geom_line(color = "green4") +
  xlab(expression(lambda)) +
  ylab("variance") +
  theme(plot.title = element_text(hjust = 0.5))
```



## f)
Lastly, we are interested in the expected MSE at $\bfx_0$, $\E((y_0 - \tilde{f}(\bfx_0))^2)$, which, when we know the squared bias and the variance from **e)**, is given as $(\text{bias})^2 + \Var(\tilde{f}(\bfx_0)) + \sigma^2$, because the irreducible error is $\sigma^2$. The plot of the expected MSE, the squared bias and the variance is shown in Figure \ref{fig:expMSE}, and by using `lambdas[which.min(exp_mse)]` we find that the minimal expected MSE is found when $\lambda \approx `r lambdas[which.min(BIAS + VAR + sigma^2)]`$. That is, it is possbile to move to $\lambda > 0$, and taking a little bias, and reducing the variance, leading to a reduction in the expected MSE.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:expMSE}The expected MSE $\\E((y_0 - \\tilde{f}(\\bfx_0))^2)$ as a function of $\\lambda$, together with the squared bias and the variance."}
exp_mse <- BIAS + VAR + sigma^2

cols <- c("exp_mse" = "blue", "bias" = "red", "variance" = "green4")
dfAll <- data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)

ggplot(dfAll) +
  geom_line(aes(x = lambda, y = exp_mse, color = "exp_mse")) +
  geom_line(aes(x = lambda, y = bias, color = "bias")) +
  geom_line(aes(x = lambda, y = var, color = "variance")) +
  xlab(expression(lambda)) +
  ylab(expression(E(MSE))) +
  theme(legend.title = element_blank()) +
  scale_color_manual(values = cols)
```



# Problem 2

Read the file:
```{r, echo=TRUE, eval=TRUE}
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
                             id), header = T)
```
## a) 

The number of deceased and non-deceased:

```{r, echo=TRUE, eval=TRUE}
deceased_count <- d.corona %>%
  group_by(deceased) %>%
  summarise(n())  %>%   rename(
    c("count"="n()")
  )

table(deceased_count)
```


For each country the number of males and females:

```{r, echo=TRUE, eval=TRUE}
country_count_sex <- d.corona %>%
  group_by(country, sex) %>%
  summarise(n()) %>%
  pivot_wider(names_from = sex, values_from = `n()`)

table(country_count_sex)
```


The number of deceased and non-deceased for each sex:

```{r, echo=TRUE, eval=TRUE}
sex_count_deceased <- d.corona %>%
  group_by(deceased, sex) %>%
  summarise(n()) %>%
  pivot_wider(names_from = deceased, values_from = `n()`) %>%
  rename(
    c("non-deceased"='0', "deceased"='1')
  )

table(sex_count_deceased)
```


The number of deceased and non-deceased in France, separate for each sex:

```{r, echo=TRUE, eval=TRUE}
sex_count_deceased_france <- d.corona %>% 
  filter(country == "France") %>%
  group_by(deceased, sex) %>%
  summarise(n()) %>%
  pivot_wider(names_from = deceased, values_from = `n()`) %>%
  rename(
    c("non-deceased"='0', "deceased"='1')
  )

table(sex_count_deceased_france)
```

## b)

We fit a logistic regression model to the data and print out the summary for reference when answering the questions.

```{r, echo=TRUE, eval=TRUE}
deceased_logit=glm(deceased~sex+age+country, data=d.corona, family=binomial)
summary(deceased_logit)
```

### (i)

We create a dataframe containing the values of the covariates, and use the predict function to get the probability:

```{r, echo=TRUE, eval=TRUE}
korea_male_75 = data.frame(sex="male", age=75, country="Korea")
p_korea_male_75 = predict(deceased_logit, newdata=korea_male_75, interval='predict', type="response")
```

The probability of a 75 year old Korean male to die from Covid is thus: `r p_korea_male_75`

### (ii)

We see that the model has a dummy variable sexmale with an estimated positive coefficient of `r deceased_logit$coefficients[2]`. The p-value of this coefficient estimate is small. So there is evidence that males have higher probability of dying that females.

### (iii)

We see that the model has dummy variables: countryindonesia, countryjapan and countryKorea, such that France serves as the base line for the model. The differences we observer are:

- Residents of Indonesia are found to have a smaller probability of dying from Covid than residents of France, but the finding is not significant (high p-value). 
- Residents of Japan are found to have a smaller probability of dying from Covid than residents of France, and the finding is significant (very low p-value).
- Residents of Korea are found to have a smaller probability of dying from Covid than residents of France, and the finding is significant (low p-value).

In summary one must therefore conclude that there is evidence that country of residence influences the probability to decease.

### (iv)

The estimated coefficient of age is `r deceased_logit$coefficients[3]` and it is found to be significant. We know that the log-odds is proportional to the linear expression of coefficients and covariates. Thus, increasing a persons age by 10 years would increase the log-odds by `r 10.0*deceased_logit$coefficients[3]`. The increase in odds would thus be by a factor: `r exp(10.0*deceased_logit$coefficients[3])`.

## c)

### (i)

To answer the question we fit a model using covariates: sex, age and the interaction sex * age.

```{r, echo=TRUE, eval=TRUE}
deceased_sex_age_logit=glm(deceased~sex*age, data=d.corona, family=binomial)
summary(deceased_sex_age_logit)
```

We see that the p-value of the coefficient sexmale:age has a very high p-value. So there is no evidence for age being a greater risk factor for males compared to females.

### (ii)

To answer the question we fit a model using covariates: country, age and the interaction country * age. It should not matter that data for Japan and Korea are used in the fitting of the model, as they will not affect the coefficients we are interested in.

```{r, echo=TRUE, eval=TRUE}
deceased_country_age_logit=glm(deceased~country*age, data=d.corona, family=binomial)
summary(deceased_country_age_logit)
```

We find some evidence for age being a smaller risk factor for residents of Indonesia than for residents of France. The p-value is around 0.03. Further, we see that age is a smaller risk factor for residents of Japan and Korea compared to residents of France, although these results are not as significant. Since all three comparisons speak to residents of France having a higher risk factor (to varying strengths), and we have not made too many trials here, we conclude that there is some evidence for age being a smaller risk factor for residents of Indonesia than for residents of France.

## d)

True, True, True, False.

<!-- (i) TRUE. (We interpret the "null rate" as how often you would be wrong if you always predicted the majority class. From numbers quoted in exercise 2 a) we see that the fraction of deceased in the samples is: $\frac{105}{105+1905} = 0.0522$, so the "null rate" for misclassification would therefore be 5.22% if we only predicted non-deceased.) -->

<!-- (ii) TRUE. The model has labeled all the samples in the training data as non-deceased. This is not a very useful classifier. -->

<!-- (iii) TRUE.  -->
<!-- ```{r, echo=TRUE, eval=TRUE} -->
<!-- deceased_lda = lda(deceased~sex+age+country, data=d.corona) -->
<!-- deceased_lda_prediction = predict(deceased_lda, d.corona) -->
<!-- names(deceased_lda_prediction) -->
<!-- table(deceased_lda_prediction$class, d.corona$deceased) -->
<!-- ``` -->
<!-- $\frac{1905}{1905+0}$ -->

<!-- (iv) FALSE. -->
<!-- ```{r, echo=TRUE, eval=TRUE} -->
<!-- deceased_qda = qda(deceased~sex+age+country, data=d.corona) -->
<!-- deceased_qda_prediction = predict(deceased_qda, d.corona) -->
<!-- names(deceased_qda_prediction) -->
<!-- table(deceased_qda_prediction$class, d.corona$deceased) -->
<!-- ``` -->
<!-- QDA sensitivity: $\frac{21}{21+84}$ -->
<!-- LDA sensitivity: $\frac{0}{105+0}$ -->


# Problem 3

```{r, eval=T}
#read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO" # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```


## a)

```{r}
logReg = glm(diabetes~., data = train, family = "binomial")
summary(logReg)
```

### (i) 

For simplicity we write $\beta_0 + \beta_1x_{i1}  + \dots + \beta_7 x_{i7} = \mathbf{x}^\top \bfbeta$, where $\bfbeta = (\beta_0 \Hquad \cdots \Hquad \beta_7)^\top$ and $\mathbf{x} = (1\Hquad x_1 \Hquad \cdots \Hquad  x_7 )^\top$. Furthermore let $P(y_i = 1| \mathbf{X}={\mathbf{x}}_i) = p_i$. Starting from the expression for $p_i$ we get

\begin{align*}
 p_i &= \frac{e^{\mathbf{x}^\top \bfbeta}}{1+e^{\mathbf{x}^\top \bfbeta}} \\
 p_i + p_ie^{\mathbf{x}^\top \bfbeta} &= e^{\mathbf{x}^\top \bfbeta} \\
 p_i &= e^{\mathbf{x}^\top \bfbeta}(1-p_i) \\
 \frac{p_i}{1-p_i} &= e^{\mathbf{x}^\top \bfbeta}\\
 \log\left(\frac{p_i}{1-p_i}\right)  &= \mathbf{x}^\top \bfbeta = \beta_0 + \beta_1x_{i1}  + \dots + \beta_7 x_{i7}. \quad \blacksquare
\end{align*}

### (ii)
 
```{r, fig.width=2.5, fig.height=1.5}
logReg.prob = predict(logReg, newdata = test , type="response")
logReg.pred = ifelse(logReg.prob >0.5, "1", "0")
c=table("predicted" = logReg.pred, "true" = test$diabetes)

cm = conf_mat(c)

autoplot(cm, type = "heatmap") + ggtitle("Confusion table Logistic Regression")
```

From the table above we find $\text{sensitivity} = \frac{48}{29+48} = \frac{48}{77} = 0.623$, and $\text{specificity} = \frac{137}{137+18} = \frac{137}{155}=0.884$.



## b)

### (i)

$\pi_k$ is the prior probability for class k, i.e $P(Y=k)$, so for the diabetes problem, $\pi_1$ is the probability for a random woman of having diabetes, while $\pi_0$ is the probability of not having diabetes. $\boldsymbol{\mu}_k$ is the mean vector for each class, so in this case $\mu_1$ and $\mu_0$ will be the mean values of each predictors for a correspondingly diabetic or non-diabetic person. $\boldsymbol{\Sigma}$ is the covariance matrix between the predictors and is in LDA assumed to be the same for all classes. $f_k(x)$ is the conditional distribution of the predictors of each class, and is assumed to be normal distribution, with mean $\boldsymbol{\mu_k}$ and covariance matrix $\boldsymbol{\Sigma}$.


### (ii)

```{r, fig.width=2.5, fig.height=1.5}

lda.model = lda(diabetes~., data=train)
qda.model = qda(diabetes~., data=train)

lda.modelProb = predict(lda.model, newdata = test)$posterior
lda.modelPred = predict(lda.model, newdata = test)$class
lc = table(predicted = lda.modelPred, true =test$diabetes)

qda.modelProb = predict(qda.model, newdata = test)$posterior
qda.modelPred = predict(qda.model, newdata = test)$class
qc = table(predicted= qda.modelPred,true= test$diabetes)

cml = conf_mat(lc)
cmq = conf_mat(qc)

autoplot(cml, type = "heatmap") + ggtitle("Confusion table LDA")
autoplot(cmq, type = "heatmap") + ggtitle("Confusion table QDA")
```

The difference between LDA and QDA models, is that in LDA, one assumes that the covariance matrices is the same for all classes ($\Sigma_k = \Sigma$). By cancellations this leads to a linear decision boundary (discriminant function): $$\delta_k(\mathbf{x}) = {\mathbf{x}}^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k - \frac{1}{2}\boldsymbol{\mu}_k^\top \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k + \log \pi_k.$$ In QDA one does not assume that the covariance matrices is the same for all classes, and thus we do not obtain the same cancellation. This gives us a quadratic decision boundary $$\delta_k(\mathbf{x}) = -\frac{1}{2}\mathbf{x}^\top\boldsymbol{\Sigma}_k^{-1}\mathbf{x} + {\mathbf x}^\top \boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k - \frac{1}{2}\boldsymbol\mu_k^\top \boldsymbol{\Sigma}_k^{-1}\boldsymbol\mu_k - \frac{1}{2}\log|\boldsymbol{\Sigma}_k| + \log \pi_k.$$

## c)

### (i)
When classifying a new observation in the KNN approach, one first identifies the $k$ nearest points in the training data, measured in the euclidean metric. Then the observation is classified to the majority class amongst those $k$ nearest points, i.e. classified to the class with the highest proportion of members among the $k$ points in question. 

### (ii)
The tuning parameter $k$ can be chosen by looking at the test error (CV-error, if using cross-validation) for a range of different values of $k$. Then choose the value that gives the lowest error. 

### (iii)

```{r, fig.width=3, fig.height=1.5}
knnMod = knn(train = train, test = test, cl = train$diabetes, k = 25, prob = T)
kc = table(knnMod, test$diabetes)

cmk = conf_mat(kc)
autoplot(cmk, type = "heatmap") + ggtitle("Confusion table KNN, k = 25")
```

We then get: Sensitivity $= \frac{41}{36+41} = \frac{41}{77} = 0.532$ and specificity $= \frac{144}{144+11} = \frac{144}{155} = 0.929$.

## d)

```{r,fig.cap="\\label{fig:ROC} ROC curve for Logistic Regression, LDA, QDA and KNN, measured on the diabetes data set."}

knnModProb = attributes(knnMod)$prob
knnModProb = ifelse(knnMod == 0, 1 - knnModProb,knnModProb)

glmroc = roc(response = test$diabetes, predictor = logReg.prob)
ldaroc = roc(response = test$diabetes, predictor = lda.modelProb[,2])
qdaroc = roc(response = test$diabetes, predictor = qda.modelProb[,2])
knnroc = roc(response = test$diabetes, predictor = knnModProb)

dat = data.frame(diabetes = test$diabetes, glm = logReg.prob, lda = lda.modelProb[,2],
                 qda = qda.modelProb[,2], knn = knnModProb)
dat_long = melt_roc(dat, "diabetes", c("glm", "lda", "qda", "knn"))
ggplot(dat_long, aes(d = D, m = M, color = name)) +
  geom_roc(n.cuts=F,size=0.5) +
  xlab("1-Specificity") +
  ylab("Sensitivity")

auc(glmroc)
auc(ldaroc)
auc(qdaroc)
auc(knnroc)
```

We observe that the LDA model has the highest AUC value, $0.849$, followed by the Logistic Regression model, AUC $= 0.8451$. Thus measured in the AUC metric, we would say that LDA performs the best. Since the difference in AUC value between Logistic Regression and LDA is relatively small, we would prefer Logistic Regression for interpretability.

# Problem 4

## a)
We wish to show that for the linear regression model, the LOOCV statistic is given by
$$
  \mathrm{CV} = \frac{1}{N} \sum_{i=1}^N \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2,
$$
where $h_i = \bfx_i^\top (X^\top X)^{-1} \bfx_i$, and $\bfx_i^\top$ is the $i$-th row of $X$.

Generally we can write $\mathrm{CV} = \sum_{i=1}^N e_{(-i)}^2 / N$, where $e_{(-i)} = y_i - \hat{y}_{(-i)}$. We use the notation $A_{(-i)}$ to symbolize that element $i$ is removed from $A$ if it is a vector, and row $i$ is removed from $A$ if it is a matrix. For a linear regression model $\bfY = X \bfbeta + \varepsilon$, the estimate of $\bfbeta$ without the $i$-th case is
$$
  \hat{\bfbeta}_{(-i)} = (X_{(-i)}^\top X_{(-i)})^{-1} X_{(-i)}^\top \bfY_{(-i)}.
$$
From what we then know, we may write $e_{(-i)} = y_i - \bfx_i^\top \hat{\bfbeta}_{(-i)}$. We want another expression for $\hat{\bfbeta}_{(-i)}$, and using the Sherman-Morrison formula,
$$
  (X_{(-i)}^\top X_{(-i)})^{-1} = (X^\top X - \bfx_i \bfx_i^\top)^{-1} = (X^\top X)^{-1} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - \bfx_i (X^\top X)^{-1} \bfx_i^\top}.
$$
By the definition of $h_i$, we then get that
$$
  (X_{(-i)}^\top X_{(-i)})^{-1} = (X^\top X)^{-1} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i}.
$$
It is also clear that $X_{(-i)}^\top \bfY_{(-i)} = X^\top \bfY - \bfx_i y_i$, and thus
$$
  \hat{\bfbeta}_{(-i)} = (X_{(-i)}^\top X_{(-i)})^{-1} X_{(-i)}^\top \bfY_{(-i)} = \left[ (X^\top X)^{-1} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i} \right] (X^\top \bfY - \bfx_i y_i).
$$
Multiplying out this expression we then get
\begin{align*}
  \hat{\bfbeta}_{(-i)} &= (X^\top X)^{-1} X^\top \bfY + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i} X^\top \bfY - (X^\top X)^{-1} \bfx_i y_i - \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i} \bfx_i y_i
  \\
  &= \hat{\bfbeta} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top \hat{\bfbeta}}{1 - h_i} - (X^\top X)^{-1} \bfx_i y_i - \frac{(X^\top X)^{-1} \bfx_i h_i}{1 - h_i} y_i
  \\
  &= \hat{\bfbeta} + \frac{(X^\top X)^{-1} \bfx_i}{1 - h_i} \left[ \bfx_i^\top \hat{\bfbeta} - y_i (1 - h_i) - h_i y_i \right]
  \\
  &= \hat{\bfbeta} + \frac{(X^\top X)^{-1} \bfx_i}{1 - h_i} \left( \hat{y}_i - y_i \right) = \hat{\bfbeta} - \frac{(X^\top X)^{-1} \bfx_i}{1 - h_i} e_i,
\end{align*}
where we let $e_i = y_i - \hat{y}_i$. This allows us to find
\begin{align*}
  e_{(-i)} &= y_i - \bfx_i^\top \hat{\bfbeta}_{(-i)} = y_i - \bfx_i^\top \hat{\bfbeta} + \frac{\bfx_i^\top (X^\top X)^{-1} \bfx_i}{1 - h_i} e_i = e_i + \frac{h_i}{1 - h_i} e_i = \frac{e_i}{1 - h_i}.
\end{align*}
It then follows that
$$
  \mathrm{CV} = \frac{1}{N} \sum_{i=1}^N e_{(-i)}^2 = \frac{1}{N} \sum_{i=1}^N \left( \frac{e_i}{1 - h_i} \right)^2 = \frac{1}{N} \sum_{i=1}^N \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2,
$$
which was what to be shown. $\boldsymbol{Q.E.D.}$


## b)
False, True, True, False.


# Problem 5

Read the file:
```{r, echo=TRUE, eval=TRUE}
id <- "19auu8YlUJJJUsZY8JZfsCTWzDm6doE7C" # google file ID
d.bodyfat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
                              id), header = T)
```

## a)
```{r, echo=TRUE, eval=TRUE}
bodyfat_lm = lm(d.bodyfat$bodyfat~d.bodyfat$age+d.bodyfat$weight+d.bodyfat$bmi)
```

The $R^2$-value is: `r summary(bodyfat_lm)$r.squared`

## b)

### (i)
```{r, echo=TRUE, eval=TRUE}
bootstrap_R2 <- function(data, n_samples) {
  
  indecies <- c(1:n_samples)
  r2_values <- c(1:n_samples)
  
  for (i in indecies) {
    samples <- data[sample(nrow(data), size=nrow(data), replace=TRUE), ]
    samples_lm <- lm(samples$bodyfat~samples$age+samples$weight+samples$bmi)
    r2_values[i] <- summary(samples_lm)$r.squared
  }
  return(r2_values)
}

set.seed(4268)
n_bootstrap_samples <- 1000
r2_values <- bootstrap_R2(d.bodyfat, 1000)
```

### (ii)
```{r, echo=TRUE, eval=TRUE}
hist(r2_values, breaks = 20)
```

### (iii)
```{r, echo=TRUE, eval=TRUE}
est_mean <- mean(r2_values)
rse <- (sum((est_mean - r2_values)^2)/(n_bootstrap_samples-1))^0.5
```

The estimated mean is: `r est_mean` and residual standard error is: `r rse`.
A 95% confidence interval is therefore: [`r est_mean-2*rse`, `r est_mean+2*rse`].

### (iv)

We see that the fraction of variance explained by the model is estimated to be: `r est_mean` and that there is a 95% probability that the true fraction of variance explained by the model lies in the interval [`r est_mean-2*rse`, `r est_mean+2*rse`]. Note that the $R^2$ value from a) falls within this interval.
