---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 4"
author: "Ã˜ystein Alvestad, Lars Evje and William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=4, fig.height=3)

```

\newcommand{\E}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\bfbeta}{\boldsymbol{\beta}}

\newcommand{\Hquad}{\hspace{0.5em}}

```{r rpackages, eval=FALSE,echo=FALSE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("ggplot2") #plotting with ggplot
install.packages("ggfortify")  
install.packages("MASS")
install.packages("class")
install.packages("pROC")
install.packages("plotROC")
```



# Problem 1
We consider the regression problem
$$
  Y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon,
$$
where $\E(\varepsilon)=0$ and $\Var(\varepsilon)=\sigma^2$. We define $\bfx, \bfbeta \in \RR^{p+1}$ such that $f(\bfx) = \bfx^\top \bfbeta = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$.

## a)
We consider the estimator
$$
  \tilde{\bfbeta} = (X^\top X + \lambda I)^{-1} X^\top \bfY,
$$
for $\bfbeta$. Here $X$ is the design matrix, $\bfY$ is the response vector, $I$ is the identity matrix, and $\lambda \geq 0$ is a constant. We recall that for a constant matrix $A$ of appropriate dimensions, and a random vector $\bfZ$, we have that
\begin{equation} \label{eq:RulesForExpectationAndCovOfMatrix}
  \E(A \bfZ) = A \E(\bfZ)
  \quad \text{and} \quad
  \Var(A \bfZ) = A \E(\bfZ) A^\top.
\end{equation}
First we now find the expected value
$$
  \E(\tilde{\bfbeta}) = \E((X^\top X + \lambda I)^{-1} X^\top \bfY) = (X^\top X + \lambda I)^{-1} X^\top \E(\bfY),
$$
and because $\E(\bfY) = X \bfbeta$, we get that
$$
  \E(\tilde{\bfbeta}) =  (X^\top X + \lambda I)^{-1} X^\top X \bfbeta.
$$
Similarly, the variance-covariance matrix is
$$
  \Var(\tilde{\bfbeta}) = \Var((X^\top X + \lambda I)^{-1} X^\top \bfY) = (X^\top X + \lambda I)^{-1} X^\top \Var(\bfY) [(X^\top X + \lambda I)^{-1} X^\top]^\top,
$$
and because $\Var(\bfY) = \sigma^2 I$, we get that
$$
  \Var(\tilde{\bfbeta}) = \sigma^2 (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1},
$$
where we used $(B^{-1})^\top = (B^\top)^{-1}$, for an invertible matrix $B$.


## b)
We now let $\tilde{f}(\bfx_0) = \bfx_0^\top \tilde{\bfbeta}$ be the prediction at a new covariate vector $\bfx_0$, and we wish the expected value and variance of this. Using what we learned in **a)** and Equation \eqref{eq:RulesForExpectationAndCovOfMatrix}, we find that
$$
  \E(\tilde{f}(\bfx_0)) = \E(\bfx_0^\top \tilde{\bfbeta}) = \bfx_0^\top \E(\tilde{\bfbeta}) = \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta.
$$
Similarly, we find that
\begin{equation} \label{eq:variance}
  \Var(\tilde{f}(\bfx_0)) = \Var(\bfx_0^\top \tilde{\bfbeta}) = \bfx_0^\top \Var(\tilde{\bfbeta}) \bfx_0 = \sigma^2 \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \bfx_0.
\end{equation}


## c)
We want to find the expected MSE at $\bfx_0$, and this is most easily done using the relationship between the expected value and the variance-covariance matrix,
$$
  \E((y_0 - \tilde{f}(\bfx_0))^2) = \E(\tilde{f}(\bfx_0) - f(\bfx_0))^2 + \Var(\tilde{f}(\bfx_0)) + \Var(\varepsilon).
$$
The last two terms are known to us now, so we look further at the first term, the squared bias,
\begin{equation} \label{eq:squaredBias}
  \begin{split}
    \E(\tilde{f}(\bfx_0) - f(\bfx_0))^2 &= [\E(\tilde{f}(\bfx_0)) - \E(f(\bfx_0))]^2 = [\E(\tilde{f}(\bfx_0)) - f(\bfx_0)]^2
    \\
    &= [\bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta - \bfx_0^\top \bfbeta]^2.
  \end{split}
\end{equation}
We then get that
\begin{align*}
  \E((y_0 - \tilde{f}(\bfx_0))^2) &= \sigma^2 + [\bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta - \bfx_0^\top \bfbeta]^2
  \\
  &\phantom{=} + \sigma^2 \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \bfx_0.
\end{align*}


## d)
We start by importing the relevant quantities, as given in the project description.
```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # Google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

X <- values$X
x0 <- values$x0
beta <- values$beta
sigma <- values$sigma
```
We may then calculate the squared bias $\E(\tilde{f}(\bfx_0) - f(\bfx_0))^2$, using Equation \eqref{eq:squaredBias}. Plotting this for $\lambda \in [0, 2]$, we get the result as in Figure \ref{fig:bias}. We see that the squared bias increases with large $\lambda > 0.5$, and has a minimum when $\lambda = 0$ and $\lambda \approx 0.5$. This is expected for $\lambda = 0$, because we then get that $\tilde{\bfbeta}$ is equal to the OLS estimator. The bias measures how good the estimator is in estimating the real value, so this makes sense. From the figure it also appears that for $\lambda \approx 0.5$, the estimator $\tilde{\bfbeta}$ us estimating the real value $\bfbeta$ very good. For increasing $\lambda$ after this, the estimator seems to do a worse job in estimating $\bfbeta$.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:bias}The squared bias $\\E(\\tilde{f}(\\bfx_0) - f(\\bfx_0))^2$ as a function of $\\lambda$."}
library(ggplot2)
bias <- function(lambda, X, x0, beta) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- (t(x0) %*% inv %*% t(X) %*% X %*% beta - t(x0) %*% beta)^2
  return(value)
}

lambdas <- seq(0, 2, length.out = 500)
BIAS <- rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  BIAS[i] <- bias(lambdas[i], X, x0, beta)
}
dfBias <- data.frame(lambdas = lambdas, bias = BIAS)

ggplot(dfBias, aes(x = lambdas, y = bias)) +
  geom_line(color = "red") +
  xlab(expression(lambda)) +
  ylab(expression(bias^2))
```


## e)
We are now interested in $\Var(\tilde{f}(\bfx_0))$, which can be calculated using Equation \eqref{eq:variance}. Using $\lambda \in [0, 2]$, the result is plotted in Figure \ref{fig:variance}. We notice that the variance is decreasing for increasing values of $\lambda$. That is, as long as $\lambda$ increases, the model is less prone to overfitting. Letting $\lambda \to \infty$, we also see that the variance tends to zero.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:variance}The variance $\\Var(\\tilde{f}(\\bfx_0))$ as a function of $\\lambda$."}
variance <- function(lambda, X, x0, sigma) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- sigma^2 * t(x0) %*% inv %*% t(X) %*% X %*% inv %*% x0
  return(value)
}

lambdas <- seq(0, 2, length.out = 500)
VAR <- rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  VAR[i] <- variance(lambdas[i], X, x0, sigma)
}
dfVar <- data.frame(lambdas = lambdas, var = VAR)

ggplot(dfVar, aes(x = lambdas, y = var)) +
  geom_line(color = "green4") +
  xlab(expression(lambda)) +
  ylab("variance") +
  theme(plot.title = element_text(hjust = 0.5))
```



## f)
Lastly, we are interested in the expected MSE at $\bfx_0$, $\E((y_0 - \tilde{f}(\bfx_0))^2)$, which, when we know the squared bias and the variance from **e)**, is given as $(\text{bias})^2 + \Var(\tilde{f}(\bfx_0)) + \sigma^2$, because the irreducible error is $\sigma^2$. The plot of the expected MSE, the squared bias and the variance is shown in Figure \ref{fig:expMSE}, and by using `lambdas[which.min(exp_mse)]` we find that the minimal expected MSE is found when $\lambda \approx `r lambdas[which.min(BIAS + VAR + sigma^2)]`$. That is, it is possbile to move to $\lambda > 0$, and taking a little bias, and reducing the variance, leading to a reduction in the expected MSE.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:expMSE}The expected MSE $\\E((y_0 - \\tilde{f}(\\bfx_0))^2)$ as a function of $\\lambda$, together with the squared bias and the variance."}
exp_mse <- BIAS + VAR + sigma^2

cols <- c("exp_mse" = "blue", "bias" = "red", "variance" = "green4")
dfAll <- data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)

ggplot(dfAll) +
  geom_line(aes(x = lambda, y = exp_mse, color = "exp_mse")) +
  geom_line(aes(x = lambda, y = bias, color = "bias")) +
  geom_line(aes(x = lambda, y = var, color = "variance")) +
  xlab(expression(lambda)) +
  ylab(expression(E(MSE))) +
  theme(legend.title = element_blank()) +
  scale_color_manual(values = cols)
```


# Problem 2

## a)

## b)

## c)

## d)


# Problem 3

```{r, eval=T}
#read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO" # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```


## a)

```{r}
logReg = glm(diabetes~., data = train, family = "binomial")
summary(logReg)
```

i) 

For simplicity we write $\beta_0 + \beta_1x_{i1}  + \dots + \beta_7 x_{i7} = \mathbf{x}^T \beta$, where $\beta = (\beta_0 \Hquad \dots \Hquad \beta_7)^T$ and $\mathbf{x} = (1\Hquad x_1 \Hquad \dots \Hquad  x_7 )^T$). Furthermore let $P(y_i = 1| \mathbf{X}={\mathbf{x}}_i) = p_i$. Starting from the expression for $p_i$ we get


\begin{align*}
 p_i &= \frac{e^{\mathbf{x}^T \beta}}{1+e^{\mathbf{x}^T \beta}} \\
 p_i + p_ie^{\mathbf{x}^T \beta} &= e^{\mathbf{x}^T \beta} \\
 p_i &= e^{\mathbf{x}^T \beta}(1-p_i) \\
 \frac{p_i}{1-p_i} &= e^{\mathbf{x}^T \beta}\\
 log(\frac{p_i}{1-p_i})  &= \mathbf{x}^T \beta = \beta_0 + \beta_1x_{i1}  + \dots + \beta_7 x_{i7} \quad \blacksquare
\end{align*}

ii)
 
```{r}
logReg.prob = predict(logReg, newdata = test , type="response")
logReg.pred = ifelse(logReg.prob >0.5, "1", "0")
c=table("predicted" = logReg.pred, "true" = test$diabetes)

library(yardstick)
library(ggplot2)

cm = conf_mat(c)

autoplot(cm, type = "heatmap") + ggtitle("Confusion table Logistic Regression")
```

From the table above we find $\text{sensitivity} = \frac{48}{77} = 0.623$, and $\text{specificity} = \frac{137}{155}=0.884$.



## b)

(i) $\pi_k$ is the prior probability for class k, i.e $Pr(Y=k)$, so for the diabetes problem, $\pi_1$ is the probability for a random women of having diabetes, while $\pi_0$ is the probability of not having diabetes. $\boldsymbol{\mu}_k$ is the mean vector for each class, so in this case $\mu_1$ and $\mu_0$ will be the mean values of each predictors for a correspondingly diabetic or non diabetic person. $\boldsymbol{\Sigma}$ is the covariance matrix between the predictors and is in LDA assumed to be the same for all classes. $f_k(x)$ is the conditional distribution of the predictors of each class, and is assumed to be normal distribution, with mean $\boldsymbol{\mu_k}$ and covariance matrix $\boldsymbol{\Sigma}$.



(ii)

```{r}
library(MASS)

lda.model = lda(diabetes~., data=train)
qda.model = qda(diabetes~., data=train)

lda.modelProb = predict(lda.model, newdata = test)$posterior
lda.modelPred = predict(lda.model, newdata = test)$class
lc = table(predicted = lda.modelPred, true =test$diabetes)

qda.modelProb = predict(qda.model, newdata = test)$posterior
qda.modelPred = predict(qda.model, newdata = test)$class
qc = table(predicted= qda.modelPred,true= test$diabetes)

cml = conf_mat(lc)
cmq = conf_mat(qc)

autoplot(cml, type = "heatmap") + ggtitle("Confusion table LDA")
autoplot(cmq, type = "heatmap") + ggtitle("Confusion table QDA")
```

The difference between LDA and QDA models, is that in LDA, one assumes that the covariance matrices is the same for all classes ($\Sigma_k = \Sigma$). By cancellations this leads to a linear decision boundary (discriminant function: $$\delta_k(\bf{x}) = {\bf x}^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_k - \frac{1}{2}\boldsymbol\mu_k^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_k + \log \pi_k$$). In QDA one does not assume that the covariance matrices is the same for all classes, and thus we do not obtain the same cancellation. This gives us a quadratic decision boundary ($\delta_k(\bf{x}) = -\frac{1}{2}\bf{x}^T\boldsymbol{\Sigma_k}^{-1}\bf{x} + {\bf x}^T \boldsymbol{\Sigma_k}^{-1}\boldsymbol\mu_k - \frac{1}{2}\boldsymbol\mu_k^T \boldsymbol{\Sigma_k}^{-1}\boldsymbol\mu_k - \frac{1}{2}\log|\boldsymbol{\Sigma_k}| + \log \pi_k$).

## c)

i) When classifying a new observation in the KNN approach, one first identifies the $k$ nearest points in the training data, measured in the euclidean metric. Then the observation is classified to the majority class amongst those $k$ nearest points, i.e. classified to the class with the highest proportion of members among the $k$ points in question. 

ii) The tuning parameter $k$ can be chosen by looking at the test error (CV-error, if using cross-validation) for a range of different values of $k$.Then choose the value that gives the lowest error. 

iii)

```{r}
library(class)
knnMod = knn(train = train, test = test, cl = train$diabetes, k = 25, prob = T)
kc = table(knnMod, test$diabetes)

cmk = conf_mat(kc)
autoplot(cmk, type = "heatmap") + ggtitle("Confusion table KNN, k = 25")
```
Sensitivity $= \frac{41}{77} =  0.532$, specificity $= \frac{144}{155} = 0.929$.

## d)

```{r,fig.cap="\\label{fig:ROC} ROC curve for Logistic Regression, LDA, QDA and KNN, measured on the diabetes data set."}

knnModProb = attributes(knnMod)$prob
knnModProb = ifelse(knnMod == 0, 1 - knnModProb,knnModProb)


library(pROC)
#library(plotROC)
library(ggplot2)

glmroc = roc(response = test$diabetes, predictor = logReg.prob)
ldaroc = roc(response = test$diabetes, predictor = lda.modelProb[,2])
qdaroc = roc(response = test$diabetes, predictor = qda.modelProb[,2])
knnroc = roc(response = test$diabetes, predictor = knnModProb)

dat = data.frame(diabetes = test$diabetes, glm = logReg.prob, lda = lda.modelProb[,2], qda = qda.modelProb[,2], knn = knnModProb)
dat_long = melt_roc(dat, "diabetes", c("glm", "lda", "qda", "knn"))
ggplot(dat_long, aes(d = D, m = M, color = name)) + geom_roc(n.cuts=F,size=0.5) + xlab("1-Specificity") + ylab("Sensitivity")

auc(glmroc)
auc(ldaroc)
auc(qdaroc)
auc(knnroc)
```

We observe that the LDA model has the highest AUC value, $0.849$, followed by the Logistic Regression model, AUC $= 0.8451$. Thus measured in the AUC metric, we would say that LDA performs the best. Since the difference in AUC value between Logistic Regression and LDA is relatively small, we would prefer Logistic Regression for interpretability.

# Problem 4

## a)
We wish to show that for the linear regression model $Y = X \beta + \varepsilon$, the LOOCV statistic is given by
$$
  \mathrm{CV} = \frac{1}{N} \sum_{i=1}^N \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2,
$$
where $h_i = \bfx_i^\top (X^\top X)^{-1} \bfx_i$, and $\bfx_i^\top$ is the $i$-th row of $X$.

Generally we can write $\mathrm{CV} = \sum_{i=1}^N e_{(-i)}^2 / N$, where $e_{(-i)} = y_i - \hat{y}_{(-i)}$. We use the notation $A_{(-i)}$ to symbolize that element $i$ is removed from $A$ if it is a vector, and row $i$ is removed from $A$ if it is a matrix. For a linear regression model $\bfY = X \bfbeta + \varepsilon$, the estimate of $\bfbeta$ without the $i$-th case is
$$
  \hat{\bfbeta}_{(-i)} = (X_{(-i)}^\top X_{(-i)})^{-1} X_{(-i)}^\top \bfY_{(-i)}.
$$
From what we then know, we may write $e_{(-i)} = y_i - \bfx_i^\top \hat{\bfbeta}_{(-i)}$. We want another expression for $\hat{\bfbeta}_{(-i)}$, and using the Sherman-Morrison formula,
$$
  (X_{(-i)}^\top X_{(-i)})^{-1} = (X^\top X - \bfx_i \bfx_i^\top)^{-1} = (X^\top X)^{-1} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - \bfx_i (X^\top X)^{-1} \bfx_i^\top}.
$$
By the definition of $h_i$, we then get that
$$
  (X_{(-i)}^\top X_{(-i)})^{-1} = (X^\top X)^{-1} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i}.
$$
It is also clear that $X_{(-i)}^\top \bfY_{(-i)} = X^\top \bfY - \bfx_i y_i$, and thus
$$
  \hat{\bfbeta}_{(-i)} = (X_{(-i)}^\top X_{(-i)})^{-1} X_{(-i)}^\top \bfY_{(-i)} = \left[ (X^\top X)^{-1} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i} \right] (X^\top \bfY - \bfx_i y_i).
$$
Multiplying out this expression we then get
\begin{align*}
  \hat{\bfbeta}_{(-i)} &= (X^\top X)^{-1} X^\top \bfY + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i} X^\top \bfY - (X^\top X)^{-1} \bfx_i y_i - \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top (X^\top X)^{-1}}{1 - h_i} \bfx_i y_i
  \\
  &= \hat{\bfbeta} + \frac{(X^\top X)^{-1} \bfx_i \bfx_i^\top \hat{\bfbeta}}{1 - h_i} - (X^\top X)^{-1} \bfx_i y_i - \frac{(X^\top X)^{-1} \bfx_i h_i}{1 - h_i} y_i
  \\
  &= \hat{\bfbeta} + \frac{(X^\top X)^{-1} \bfx_i}{1 - h_i} \left[ \bfx_i^\top \hat{\bfbeta} - y_i (1 - h_i) - h_i y_i \right]
  \\
  &= \hat{\bfbeta} + \frac{(X^\top X)^{-1} \bfx_i}{1 - h_i} \left( \hat{y}_i - y_i \right) = \hat{\bfbeta} - \frac{(X^\top X)^{-1} \bfx_i}{1 - h_i} e_i,
\end{align*}
where we let $e_i = y_i - \hat{y}_i$. This allows us to find
\begin{align*}
  e_{(-i)} &= y_i - \bfx_i^\top \hat{\bfbeta}_{(-i)} = y_i - \bfx_i^\top \hat{\bfbeta} + \frac{\bfx_i^\top (X^\top X)^{-1} \bfx_i}{1 - h_i} e_i = e_i + \frac{h_i}{1 - h_i} e_i = \frac{e_i}{1 - h_i}.
\end{align*}
It then follows that
$$
  \mathrm{CV} = \frac{1}{N} \sum_{i=1}^N e_{(-i)}^2 = \frac{1}{N} \sum_{i=1}^N \left( \frac{e_i}{1 - h_i} \right)^2 = \frac{1}{N} \sum_{i=1}^N \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2,
$$
which was what to be shown. $\boldsymbol{Q.E.D.}$


## b)
False, True, True, False.


# Problem 5

## a)

## b)