---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 4"
author: "Ã˜ystein Alvestad, Lars Evje and William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, message=FALSE, warning=FALSE, strip.white=TRUE, prompt=FALSE, cache=TRUE, size="scriptsize", fig.width=4, fig.height=3)

```

\newcommand{\E}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\bfbeta}{\boldsymbol{\beta}}

```{r rpackages, eval=FALSE,echo=FALSE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("ggplot2") #plotting with ggplot
install.packages("ggfortify")  
install.packages("MASS")
install.packages("class")
install.packages("pROC")
install.packages("plotROC")
```



# Problem 1
We consider the regression problem
$$
  Y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon,
$$
where $\E(\varepsilon)=0$ and $\Var(\varepsilon)=\sigma^2$. We define $\bfx, \bfbeta \in \RR^{p+1}$ such that $f(\bfx) = \bfx^\top \bfbeta = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$.

## a)
We consider the estimator
$$
  \tilde{\bfbeta} = (X^\top X + \lambda I)^{-1} X^\top \bfY,
$$
for $\bfbeta$. Here $X$ is the design matrix, $\bfY$ is the response vector, $I$ is the identity matrix, and $\lambda \geq 0$ is a constant. We recall that for a constant matrix $A$ of appropriate dimensions, and a random vector $\bfZ$, we have that
\begin{equation} \label{eq:RulesForExpectationAndCovOfMatrix}
  \E(A \bfZ) = A \E(\bfZ)
  \quad \text{and} \quad
  \Var(A \bfZ) = A \E(\bfZ) A^\top.
\end{equation}
First we now find the expected value
$$
  \E(\tilde{\bfbeta}) = \E((X^\top X + \lambda I)^{-1} X^\top \bfY) = (X^\top X + \lambda I)^{-1} X^\top \E(\bfY),
$$
and because $\E(\bfY) = X \bfbeta$, we get that
$$
  \E(\tilde{\bfbeta}) =  (X^\top X + \lambda I)^{-1} X^\top X \bfbeta.
$$
Similarly, the variance-covariance matrix is
$$
  \Var(\tilde{\bfbeta}) = \Var((X^\top X + \lambda I)^{-1} X^\top \bfY) = (X^\top X + \lambda I)^{-1} X^\top \Var(\bfY) [(X^\top X + \lambda I)^{-1} X^\top]^\top,
$$
and because $\Var(\bfY) = \sigma^2 I$, we get that
$$
  \Var(\tilde{\bfbeta}) = \sigma^2 (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1},
$$
where we used $(B^{-1})^\top = (B^\top)^{-1}$, for an invertible matrix $B$.


## b)
We now let $\tilde{f}(\bfx_0) = \bfx_0^\top \tilde{\bfbeta}$ be the prediction at a new covariate vector $\bfx_0$, and we wish the expected value and variance of this. Using what we learned in **a)** and Equation \eqref{eq:RulesForExpectationAndCovOfMatrix}, we find that
$$
  \E(\tilde{f}(\bfx_0)) = \E(\bfx_0^\top \tilde{\bfbeta}) = \bfx_0^\top \E(\tilde{\bfbeta}) = \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta.
$$
Similarly, we find that
\begin{equation} \label{eq:variance}
  \Var(\tilde{f}(\bfx_0)) = \Var(\bfx_0^\top \tilde{\bfbeta}) = \bfx_0^\top \Var(\tilde{\bfbeta}) \bfx_0 = \sigma^2 \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \bfx_0.
\end{equation}


## c)
We want to find the expected MSE at $\bfx_0$, and this is most easily done using the relationship between the expected value and the variance-covariance matrix,
$$
  \E((y_0 - \tilde{f}(\bfx_0))^2) = \E(\tilde{f}(\bfx_0) - f(\bfx_0))^2 + \Var(\tilde{f}(\bfx_0)) + \Var(\varepsilon).
$$
The last two terms are known to us now, so we look further at the first term
\begin{equation} \label{eq:squaredBias}
  \begin{split}
    \E(\tilde{f}(\bfx_0) - f(\bfx_0))^2 &= [\E(\tilde{f}(\bfx_0)) - \E(f(\bfx_0))]^2 = [\E(\tilde{f}(\bfx_0)) - f(\bfx_0)]^2
    \\
    &= [\bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta - \bfx_0^\top \bfbeta]^2.
  \end{split}
\end{equation}
We then get that
\begin{align*}
  \E((y_0 - \tilde{f}(\bfx_0))^2) &= \sigma^2 + [\bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X \bfbeta - \bfx_0^\top \bfbeta]^2
  \\
  &\phantom{=} + \sigma^2 \bfx_0^\top (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \bfx_0.
\end{align*}


## d)
We start by importing the relevant quantities, as given in the project description.
```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # Google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

X <- values$X
x0 <- values$x0
beta <- values$beta
sigma <- values$sigma
```
We may then calculate the squared bias $\E(\tilde{f}(\bfx_0) - f(\bfx_0))^2$, using Equation \eqref{eq:squaredBias}. Plotting this for $\lambda \in [0, 2]$, we get the result as in Figure \ref{fig:bias}. We see that the squared bias increases with large $\lambda > 0.5$, and has a minimum when $\lambda = 0$ and $\lambda \approx 0.5$. This is expected for $\lambda = 0$, because we then get that $\tilde{\bfbeta}$ is equal to the OLS estimator. The bias measures how good the estimator is in estimating the real value, so this makes sense. From the figure it also appears that for $\lambda \approx 0.5$, the estimator $\tilde{\bfbeta}$ us estimating the real value $\bfbeta$ very good. For increasing $\lambda$ after this, the estimator seems to do a worse job in estimating $\bfbeta$.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:bias}The squared bias $\\E(\\tilde{f}(\\bfx_0) - f(\\bfx_0))^2$ as a function of $\\lambda$."}
library(ggplot2)
bias <- function(lambda, X, x0, beta) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- (t(x0) %*% inv %*% t(X) %*% X %*% beta - t(x0) %*% beta)^2
  return(value)
}

lambdas <- seq(0, 2, length.out = 500)
BIAS <- rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  BIAS[i] <- bias(lambdas[i], X, x0, beta)
}
dfBias <- data.frame(lambdas = lambdas, bias = BIAS)

ggplot(dfBias, aes(x = lambdas, y = bias)) +
  geom_line(color = "red") +
  xlab(expression(lambda)) +
  ylab(expression(bias^2))
```


## e)
We are now interested in $\Var(\tilde{f}(\bfx_0))$, which can be calculated using Equation \eqref{eq:variance}. Using $\lambda \in [0, 2]$, the result is plotted in Figure \ref{fig:variance}. We notice that the variance is decreasing for increasing values of $\lambda$. That is, as long as $\lambda$ increases, the model is less prone to overfitting. Letting $\lambda \to \infty$, we also see that the variance tends to zero.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:variance}The variance $\\Var(\\tilde{f}(\\bfx_0))$ as a function of $\\lambda$."}
variance <- function(lambda, X, x0, sigma) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- sigma^2 * t(x0) %*% inv %*% t(X) %*% X %*% inv %*% x0
  return(value)
}

lambdas <- seq(0, 2, length.out = 500)
VAR <- rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  VAR[i] <- variance(lambdas[i], X, x0, sigma)
}
dfVar <- data.frame(lambdas = lambdas, var = VAR)

ggplot(dfVar, aes(x = lambdas, y = var)) +
  geom_line(color = "green4") +
  xlab(expression(lambda)) +
  ylab("variance") +
  theme(plot.title = element_text(hjust = 0.5))
```



## f)
Lastly, we are interested in the expected MSE at $\bfx_0$, $\E((y_0 - \tilde{f}(\bfx_0))^2)$, which, when we know the squared bias and the variance from **e)**, is given as $(\text{bias})^2 + \Var(\tilde{f}(\bfx_0)) + \sigma^2$, because the irreducible error is $\sigma^2$. The plot of the expected MSE, the squared bias and the variance is shown in Figure \ref{fig:expMSE}, and by using `lambdas[which.min(exp_mse)]` we find that the minimal expected MSE is found when $\lambda \approx `r lambdas[which.min(BIAS + VAR + sigma^2)]`$. That is, it is possbile to move to $\lambda > 0$, and taking a little bias, and reducing the variance, leading to a reduction in the expected MSE.
```{r, echo=TRUE, eval=TRUE, fig.cap="\\label{fig:expMSE}The expected MSE $\\E((y_0 - \\tilde{f}(\\bfx_0))^2)$ as a function of $\\lambda$, together with the squared bias and the variance."}
exp_mse <- BIAS + VAR + sigma^2

cols <- c("exp_mse" = "blue", "bias" = "red", "variance" = "green4")
dfAll <- data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)

ggplot(dfAll)+
  geom_line(aes(x = lambda, y = exp_mse, color = "exp_mse")) +
  geom_line(aes(x = lambda, y = bias, color = "bias")) +
  geom_line(aes(x = lambda, y = var, color = "variance")) +
  xlab(expression(lambda)) +
  ylab(expression(E(MSE))) +
  theme(legend.title = element_blank()) +
  scale_color_manual(values = cols)
```


# Problem 2

## a)

## b)

## c)

## d)


# Problem 3

## a)

## b)

## c)

## d)



# Problem 4

## a)

## b)


# Problem 5

## a)

## b)